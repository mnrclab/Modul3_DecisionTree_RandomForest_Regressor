{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a supervised machine learning model used to predict a target by learning decision rules from features. As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions.\n",
    "\n",
    "DTs algorithms are perfect to solve classification (where machines sort data into classes, like whether an email is spam or not) and regression (where machines predict values, like a property price) problems. Regression Trees are used when the dependent variable is continuous or quantitative (e.g. if we want to estimate the probability that a customer will default on a loan), and Classification Trees are used when the dependent variable is categorical or qualitative (e.g. if we want to estimate the blood type of a person).\n",
    "\n",
    "<img src = 'p_img.png'>\n",
    "\n",
    "<img src = 'o_img.png'  width=\"600\" height=\"400\">\n",
    "\n",
    "Decision Trees are divided into Classification and Regression Trees. Regression trees are needed when the response variable is numeric or continuous. Classification trees, as the name implies are used to separate the dataset into classes belonging to the response variable. \n",
    "\n",
    "<img src='a_img.png'>\n",
    "\n",
    "### **Decision Tree Regression:**\n",
    "\n",
    "<img src='c_img.png'  width=\"600\" height=\"400\">\n",
    "\n",
    "<img src='n_img.png'>\n",
    "\n",
    "### **Decision Tree Classification:**\n",
    "\n",
    "<img src='e_img.png' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Fundamentals of Decision Trees**\n",
    "\n",
    "A decision tree is constructed by **recursive partitioning** — starting from the root node (known as the first parent), each node can be split into left and right child nodes. These nodes can then be further split and they themselves become parent nodes of their resulting children nodes.\n",
    "\n",
    "<img src = 'f_img.png'  width=\"700\" height=\"600\">\n",
    "\n",
    "For example, looking at the image above, the root node is **Work to do?** and splits into the child nodes **Stay in** and **Outlook** based on whether or not there is work to do. The **Outlook** node further splits into three child nodes.\n",
    "\n",
    "So, how do we know what the optimal splitting point is at each node?\n",
    "\n",
    "Starting from the root, the data is split on the feature that results in the largest **Information Gain (IG)**. In an iterative process, we then repeat this splitting procedure at each **child node** until the leaves are pure — i.e. samples at each node all belong to the same class.\n",
    "\n",
    "``In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically want to prune the tree by setting a limit for the maximal depth of the tree.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Maximizing Information Gain**\n",
    "\n",
    "In order to split the nodes at the most informative features, we need to define an objective function that we want to optimize via the tree learning algorithm. Here, our objective function is to maximize the information gain at each split, which we define as follows:\n",
    "\n",
    "<img src='g_img.png'>\n",
    "\n",
    "Here, f is the feature to perform the split, Dp, Dleft, and Dright are the datasets of the parent and child nodes, I is the **impurity measure**, Np is the total number of samples at the parent node, and Nleft and Nright are the number of samples in the child nodes.\n",
    "\n",
    "Just understand that information gain is simply the difference between the impurity of the parent node and the sum of the child node impurities — the lower the impurity of the child nodes, the larger the information gain.\n",
    "\n",
    "Note that the above equation is for binary decision trees — each parent node is split into two child nodes only. If you have a decision tree with multiple nodes, you would simply sum the impurity of all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How Do Decision Trees Work?**\n",
    "\n",
    "There are several steps involved in the building of a decision tree.\n",
    "\n",
    "### **1. Splitting**\n",
    "\n",
    "The process of partitioning the data set into subsets. Splits are formed on a particular variable\n",
    "\n",
    "<img src = 'j_img.png'>\n",
    "\n",
    "### **2. Pruning**\n",
    "\n",
    "The shortening of branches of the tree. Pruning is the process of reducing the size of the tree by turning some branch nodes into leaf nodes, and removing the leaf nodes under the original branch. Pruning is useful because classification trees may fit the training data well, but may do a poor job of classifying new values. A simpler tree often avoids over-fitting.\n",
    "\n",
    "<img src = 'k_img.png'>\n",
    "\n",
    "As you can see, a pruned tree has less nodes and has less sparsity than a unpruned decision tree.\n",
    "\n",
    "### **3. Tree Selection**\n",
    "\n",
    "The process of finding the smallest tree that fits the data. Usually this is the tree that yields the lowest cross-validated error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Random Forest**\n",
    "\n",
    "The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random:\n",
    "\n",
    "*    Random sampling of training data points when building trees\n",
    "*    Random subsets of features considered when splitting nodes\n",
    "\n",
    "<img src = 'm_img.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How do Random Forests work?**\n",
    "\n",
    "A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. What is bagging you may ask? Bagging, in the Random Forest method, involves training each decision tree on a different data sample where sampling is done with replacement.\n",
    "\n",
    "<img src = 'l_img.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Types of Ensemble Learning:**\n",
    "\n",
    "*    Boosting.\n",
    "*    Bootstrap Aggregation (Bagging).\n",
    "\n",
    "**1. Boosting**\n",
    "\n",
    "Boosting refers to a group of algorithms that utilize weighted averages to make weak learners into stronger learners. Boosting is all about “teamwork”. Each model that runs, dictates what features the next model will focus on.\n",
    "\n",
    "In boosting as the name suggests, one is learning from other which in turn boosts the learning.\n",
    "\n",
    "**2. Bootstrap Aggregation (Bagging)**\n",
    "\n",
    "Bootstrap refers to random sampling with replacement. Bootstrap allows us to better understand the bias and the variance with the dataset. Bootstrap involves random sampling of small subset of data from the dataset.\n",
    "\n",
    "It is a general procedure that can be used to reduce the variance for those algorithm that have high variance, typically decision trees. Bagging makes each model run independently and then aggregates the outputs at the end without preference to any model.\n",
    "\n",
    "Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression.\n",
    "\n",
    "Random forest is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\n",
    "\n",
    "It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "## **Problems with Decision Trees**\n",
    "\n",
    "Decision trees are sensitive to the specific data on which they are trained. If the training data is changed the resulting decision tree can be quite different and in turn the predictions can be quite different.\n",
    "\n",
    "Also Decision trees are computationally expensive to train, carry a big risk of overfitting, and tend to find local optima because they can’t go back after they have made a split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practice using Graduate Admissions Dataset**\n",
    "Dataset ini dibuat oleh Mohan S Acharya untuk memperkirakan peluang penerimaan lulusan dari perspektif India. Analisis kami akan membantu kami dalam memahami faktor-faktor apa yang penting dalam penerimaan lulusan dan bagaimana faktor-faktor ini saling terkait satu sama lain. Ini juga akan membantu memprediksi peluang seseorang untuk masuk mengingat variabel-variabel lainnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Import Library__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# mengabaikan warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Open Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Drop Irrelevant Feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Serial No.'], axis=1)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Splitting X dan y__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['GRE Score', 'TOEFL Score', 'CGPA', 'University Rating', 'SOP', 'LOR ', 'Research']].values\n",
    "y = df['Chance of Admit '].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Splitting Data Train & Data Test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Import Regression Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import regression models\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, BayesianRidge, ElasticNet, HuberRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ['Linear Regression : ', LinearRegression()],\n",
    "    ['ElasticNet : ', ElasticNet()],\n",
    "    ['Lasso : ', Lasso()],\n",
    "    ['Ridge : ', Ridge()],\n",
    "    ['DecisionTree : ', DecisionTreeRegressor()],\n",
    "    ['RandomForest : ', RandomForestRegressor()],\n",
    "    \n",
    "    ['KNeighbours : ', KNeighborsRegressor(n_neighbors = 2)],\n",
    "    ['SVM : ', SVR()],\n",
    "    ['AdaBoost : ', AdaBoostRegressor()],\n",
    "    ['GradientBoosting : ', GradientBoostingRegressor()],\n",
    "    ['BayesianRidge : ', BayesianRidge()],\n",
    "    ['Huber : ', HuberRegressor()],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score Linear Regression :  0.06931923665033544\n",
      "RMSE Score ElasticNet :  0.09572395283980283\n",
      "RMSE Score Lasso :  0.11515264536386086\n",
      "RMSE Score Ridge :  0.06918561801071152\n",
      "RMSE Score DecisionTree :  0.09912744322335768\n",
      "RMSE Score RandomForest :  0.07814676256889976\n",
      "RMSE Score KNeighbours :  0.09465727652959385\n",
      "RMSE Score SVM :  0.08490376711218003\n",
      "RMSE Score AdaBoost :  0.08025159242279596\n",
      "RMSE Score GradientBoosting :  0.07883754418163126\n",
      "RMSE Score BayesianRidge :  0.0691069728305366\n",
      "RMSE Score Huber :  0.0748563592834582\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    model = model\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('RMSE Score', name, (np.sqrt(mean_squared_error(y_test, predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Take Home Exercise**\n",
    "1. Dataset yang digunakan adalah __Admission_Predict.csv__. \n",
    "\n",
    "2. __Variabel__ : ``X = 'GRE Score', 'TOEFL Score','University Rating', 'SOP', 'LOR ', 'CGPA', 'Research'``, ``Y = Chance of Admit``\n",
    "\n",
    "3. __Urutan percobaan__ (train 80% test 20%):\n",
    "\n",
    "    3.a) Variabel independent ``tidak ada intervensi sama sekali`` + pilihan model (seperti yang dicoba di kelas)\n",
    "    \n",
    "    3.b) ``Handling outlier`` + pilihan model (seperti yang dicoba di kelas)\n",
    "    \n",
    "    3.c) ``Handling outlier + Scalling`` + pilihan model (seperti yang dicoba di kelas)\n",
    "    \n",
    "4. Di setiap percobaan tampilkan RMSE\n",
    "\n",
    "5. Tentukan percobaan dan model apa yang terbaik untuk memprediksi ``Chance of Admit``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reference**:\n",
    "\n",
    "* Samet Girgin, \"Decision Tree Regression in 6 Steps with Python\", https://medium.com/pursuitnotes/decision-tree-regression-in-6-steps-with-python-1a1c5aa2ee16\n",
    "* Lorraine Li, \"Classification and Regression Analysis with Decision Trees\", https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054\n",
    "* Diego Lopez Yse, \"The Complete Guide to Decision Trees\", https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\n",
    "* Chirag Sehra, \"Decision Trees Explained Easily\", https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248\n",
    "* Prashant Gupta, \"Decision Trees in Machine Learning\", https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
    "* Will Koehrsen, \"Random Forest Simple Explanation\", https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "* Afroz Chakure, \"Random Forest Regression\", https://towardsdatascience.com/random-forest-and-its-implementation-71824ced454f\n",
    "* Krishni, \"A Beginners Guide to Random Forest Regression\", https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "* Will Koehrsen, \"An Implementation and Explanation of the Random Forest in Python\", https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "* Dataset source: https://www.kaggle.com/mohansacharya/graduate-admissions/data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python38132bitf9f79e71b62e4503b25567c1d3914456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
